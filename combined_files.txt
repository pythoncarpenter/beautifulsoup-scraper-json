Files used in this run: Dockerfile run.sh scrape.log scrape.py

# Use a lightweight Python image
FROM python:3.12-slim

# Install pipenv so that run.sh can use it
RUN pip install pipenv

# Set the working directory in the container
WORKDIR /app

# Install Tkinter (python3-tk) and clean up to keep the image lean
RUN apt-get update && apt-get install -y python3-tk && rm -rf /var/lib/apt/lists/*

# Copy Pipfile and Pipfile.lock into the container
COPY Pipfile Pipfile.lock ./

# Install dependencies using pipenv
RUN pipenv install --deploy --ignore-pipfile

# Copy the rest of your application code into the container
COPY . .

# Set the default command to run the scraper script
CMD ["./run.sh"]
#!/bin/bash
# run.sh - Comprehensive one-click setup with recursive retry, network timeout handling,
# and graceful error handling for first-time execution.

# --- Enable Strict Error Handling ---
set -euo pipefail
trap 'echo "Error occurred at line ${LINENO}. Exiting." && exit 1' ERR

# --- Function: Retry pipenv install with network timeout handling ---
retry_pipenv_install() {
    local retries=3
    local count=1
    while [ $count -le $retries ]; do
        echo "Attempt $count of $retries: Running 'pipenv install'..."
        if pipenv install; then
            echo "pipenv install succeeded."
            return 0
        else
            echo "Attempt $count failed. Possible network issue. Retrying in 5 seconds..."
            sleep 5
        fi
        count=$((count + 1))
    done
    echo "pipenv install failed after $retries attempts."
    return 1
}

# --- Function: Check and wait for XQuartz on macOS (recursive) ---
check_xquartz_recursive() {
    if pgrep -x "XQuartz" > /dev/null; then
        echo "XQuartz is running."
    else
        echo "XQuartz not running. Retrying in 2 seconds..."
        sleep 2
        check_xquartz_recursive  # Recursive call until XQuartz is detected.
    fi
}

# --- Function: Setup environment and run scraper (recursive retry) ---
setup_and_run() {
    local attempt="${1:-1}"
    local max_attempts=5

    echo "Setup attempt $attempt of $max_attempts..."

    # Step 0: Ensure key files have execute permissions.
    chmod +x run.sh scrape.py setup_env.sh || echo "Warning: Could not update permissions for some files."

    # Step 1: Check for pipenv.
    if ! command -v pipenv >/dev/null 2>&1; then
        echo "Error: pipenv is not installed. Please install pipenv and try again." >&2
        exit 1
    fi

    # Step 2: Verify that Pipfile exists.
    if [ ! -f "Pipfile" ]; then
        echo "Error: Pipfile not found in the repository. Aborting." >&2
        exit 1
    fi

    # Step 3: Ensure the virtual environment exists; if not, create it.
    venv_path=$(pipenv --venv 2>/dev/null || true)
    if [ -z "$venv_path" ] || [ ! -d "$venv_path" ]; then
        echo "Virtual environment not found. Running 'pipenv install'..."
        if ! retry_pipenv_install; then
            echo "Error: 'pipenv install' failed. Aborting."
            exit 1
        fi
        # After installing, recursively call setup_and_run to re-check the environment.
        if [ "$attempt" -lt "$max_attempts" ]; then
            sleep 2
            setup_and_run $((attempt + 1))
            return
        else
            echo "Max attempts reached while creating virtual environment. Aborting."
            exit 1
        fi
    fi

    # Step 4: Check that the Python version is correct (3.12 expected).
    expected_python="3.12"
    venv_python_version=$(pipenv run python -c "import sys; print('.'.join(map(str, sys.version_info[:2])))" 2>/dev/null)
    if [ "$venv_python_version" != "$expected_python" ]; then
        echo "Error: Python version in virtual environment ($venv_python_version) does not match expected ($expected_python)." >&2
        exit 1
    fi

    # Step 5: Verify required packages are installed.
    required_packages=("pandas" "numpy" "statsmodels" "matplotlib" "openai")
    pipenv run pip list > /tmp/installed_packages.txt
    for pkg in "${required_packages[@]}"; do
        if ! grep -qi "$pkg" /tmp/installed_packages.txt; then
            echo "Error: Package '$pkg' is not installed in the virtual environment." >&2
            rm /tmp/installed_packages.txt
            exit 1
        fi
    done
    rm /tmp/installed_packages.txt
    echo "Environment checks passed."

    # Step 6: Setup system-specific GUI engine.
    os_type=$(uname)
    if [[ "$os_type" == "Darwin" ]]; then
        # On macOS, ensure XQuartz is running.
        if ! pgrep -x "XQuartz" > /dev/null; then
            echo "XQuartz is not running. Launching XQuartz..."
            open -a XQuartz || { echo "Error: Failed to launch XQuartz. Aborting."; exit 1; }
        fi
        # Use recursive function to wait for XQuartz.
        check_xquartz_recursive
        # Set DISPLAY if not already set.
        if [ -z "${DISPLAY:-}" ]; then
            echo "DISPLAY variable not set. Setting to :0.0 for GUI support."
            export DISPLAY=:0.0
        fi
    elif [[ "$os_type" == "Linux" ]]; then
        if [ -z "${DISPLAY:-}" ]; then
            echo "Warning: DISPLAY is not set. Ensure an X server is running for GUI support." >&2
        fi
    elif [[ "$os_type" == MINGW* || "$os_type" == CYGWIN* ]]; then
        echo "Windows detected. Please ensure you have a compatible GUI engine available." >&2
    fi

    # Step 7: Run the scraper application.
    echo "Starting the scraper application..."
    if [ -z "${VIRTUAL_ENV:-}" ]; then
        pipenv run python scrape.py || { echo "Error: Running scrape.py failed."; exit 1; }
    else
        python scrape.py || { echo "Error: Running scrape.py failed."; exit 1; }
    fi

    echo "Scraper application completed successfully."
    echo "Check scrape.log and the output JSON file for results."
}

# --- Kick Off the Setup Process ---
setup_and_run 1
2025-02-01 17:26:35 INFO: Submit button pressed.
2025-02-01 17:26:35 INFO: Mode selected: fixes
2025-02-01 17:26:35 INFO: Starting scrape_fix_pages for URL: https://github.com/vercel/nft/pull/455
2025-02-01 17:26:35 DEBUG: Starting new HTTPS connection (1): github.com:443
2025-02-01 17:26:36 DEBUG: https://github.com:443 "GET /vercel/nft/pull/455 HTTP/1.1" 200 None
2025-02-01 17:26:36 INFO: Received response with status code: 200
2025-02-01 17:26:36 DEBUG: HTML snippet for fix page: 






<!DOCTYPE html>
<html
  lang="en"
  
  data-color-mode="auto" data-light-theme="light" data-dark-theme="dark"
  data-a11y-animated-images="system" data-a11y-link-underlines="true"
  
  >



  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://github.githubassets.com">
...
2025-02-01 17:26:36 INFO: Found 0 fix rows
2025-02-01 17:26:36 INFO: Finished scraping fixes. Total fixes: 0
2025-02-01 17:26:36 INFO: Results successfully written to /Users/mitchmcquoid/Desktop/fixes444.json
2025-02-01 17:35:16 INFO: Submit button pressed.
2025-02-01 17:35:16 INFO: Mode selected: fixes
2025-02-01 17:35:16 INFO: Starting scrape_fix_pages for URL: https://github.com/vercel/nft/pull/455
2025-02-01 17:35:16 DEBUG: Starting new HTTPS connection (1): github.com:443
2025-02-01 17:35:17 DEBUG: https://github.com:443 "GET /vercel/nft/pull/455 HTTP/1.1" 200 None
2025-02-01 17:35:17 INFO: Received response with status code: 200
2025-02-01 17:35:17 INFO: Found 16 conversation items
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 DEBUG: Skipping an item due to missing content.
2025-02-01 17:35:17 INFO: Finished scraping fixes. Total responses: 8
2025-02-01 17:35:17 INFO: Results successfully written to /Users/mitchmcquoid/Desktop/fix2222.json
2025-02-01 18:01:32 INFO: Submit button pressed.
2025-02-01 18:01:32 INFO: Mode selected: fixes
2025-02-01 18:01:32 INFO: Starting scrape_fix_pages for URL: https://github.com/vercel/nft/issues
2025-02-01 18:01:32 DEBUG: Starting new HTTPS connection (1): github.com:443
2025-02-01 18:01:33 DEBUG: https://github.com:443 "GET /vercel/nft/issues HTTP/1.1" 200 None
2025-02-01 18:01:33 INFO: Received response with status code: 200
2025-02-01 18:01:33 INFO: Found 0 conversation items
2025-02-01 18:01:33 INFO: Finished scraping fixes. Total responses: 0
2025-02-01 18:01:33 INFO: Results successfully written to /Users/mitchmcquoid/Desktop/fixes.json
2025-02-01 18:01:56 INFO: Submit button pressed.
2025-02-01 18:01:56 INFO: Mode selected: fixes
2025-02-01 18:01:56 INFO: Starting scrape_fix_pages for URL: https://github.com/vercel/nft/pull/455
2025-02-01 18:01:56 DEBUG: Starting new HTTPS connection (1): github.com:443
2025-02-01 18:01:57 DEBUG: https://github.com:443 "GET /vercel/nft/pull/455 HTTP/1.1" 200 None
2025-02-01 18:01:57 INFO: Received response with status code: 200
2025-02-01 18:01:57 INFO: Found 16 conversation items
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:01:57 INFO: Finished scraping fixes. Total responses: 8
2025-02-01 18:01:57 INFO: Results successfully written to /Users/mitchmcquoid/Desktop/fixes999.json
2025-02-01 18:05:36 INFO: Submit button pressed.
2025-02-01 18:05:36 INFO: Mode selected: fixes
2025-02-01 18:05:36 INFO: Starting scrape_fix_pages for URL: https://github.com/vercel/nft/pull/455
2025-02-01 18:05:36 DEBUG: Starting new HTTPS connection (1): github.com:443
2025-02-01 18:05:37 DEBUG: https://github.com:443 "GET /vercel/nft/pull/455 HTTP/1.1" 200 None
2025-02-01 18:05:37 INFO: Received response with status code: 200
2025-02-01 18:05:37 INFO: Found 16 conversation items
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:05:37 INFO: Duplicate item skipped: {'id': 'e8644d62-3114-479b-a35e-1d4d048271a8', 'type': 'comment', 'author': 'benmccann', 'timestamp': '2024-12-09T22:48:48Z', 'content': 'This removes 5 dependencies from the dependency tree'}
2025-02-01 18:05:37 INFO: Duplicate item skipped: {'id': '768bae17-0954-406b-9cd3-5e019dcb98f6', 'type': 'comment', 'author': 'benmccann', 'timestamp': '2024-12-09T22:48:48Z', 'content': 'This removes 5 dependencies from the dependency tree'}
2025-02-01 18:05:37 INFO: Duplicate item skipped: {'id': '559694fe-1185-4ea6-ba67-52af1cc8253b', 'type': 'comment', 'author': 'socket-security', 'timestamp': '2024-12-09T22:56:28Z', 'content': 'New and removed dependencies detected.Learn more aboutSocket for GitHub ↗︎PackageNew capabilitiesTransitivesSizePublishernpm/@types/picomatch@3.0.1None018.1 kBtypes🚮 Removed packages:npm/@types/micromatch@4.0.6View full report↗︎'}
2025-02-01 18:05:37 INFO: Duplicate item skipped: {'id': '292da71c-ea21-481f-9d93-2338f60cc473', 'type': 'comment', 'author': 'socket-security', 'timestamp': '2024-12-09T22:56:28Z', 'content': 'New and removed dependencies detected.Learn more aboutSocket for GitHub ↗︎PackageNew capabilitiesTransitivesSizePublishernpm/@types/picomatch@3.0.1None018.1 kBtypes🚮 Removed packages:npm/@types/micromatch@4.0.6View full report↗︎'}
2025-02-01 18:05:37 INFO: Finished scraping fixes. Total unique responses: 4
2025-02-01 18:05:37 INFO: Results successfully written to /Users/mitchmcquoid/Desktop/fixes.json
2025-02-01 18:09:54 INFO: Submit button pressed.
2025-02-01 18:09:54 INFO: Mode selected: fixes
2025-02-01 18:09:54 INFO: Starting scrape_fix_pages for URL: https://github.com/vercel/nft/pull/455
2025-02-01 18:09:54 DEBUG: Starting new HTTPS connection (1): github.com:443
2025-02-01 18:09:54 DEBUG: https://github.com:443 "GET /vercel/nft/pull/455 HTTP/1.1" 200 None
2025-02-01 18:09:55 INFO: Received response with status code: 200
2025-02-01 18:09:55 INFO: Found 16 conversation items
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 DEBUG: Skipping a fix item due to missing content.
2025-02-01 18:09:55 INFO: Duplicate item skipped: {'id': 'ebfb88b5-3bf2-4362-89d5-3f5841c5c9c5', 'type': 'comment', 'author': 'benmccann', 'timestamp': '2024-12-09T22:48:48Z', 'content': 'This removes 5 dependencies from the dependency tree'}
2025-02-01 18:09:55 INFO: Duplicate item skipped: {'id': '55472a6d-cb5e-43b0-af86-1618b78f93a2', 'type': 'comment', 'author': 'benmccann', 'timestamp': '2024-12-09T22:48:48Z', 'content': 'This removes 5 dependencies from the dependency tree'}
2025-02-01 18:09:55 INFO: Duplicate item skipped: {'id': 'ccbffa3c-0b26-4c3a-aea7-af7e5244e059', 'type': 'comment', 'author': 'socket-security', 'timestamp': '2024-12-09T22:56:28Z', 'content': 'New and removed dependencies detected.Learn more aboutSocket for GitHub ↗︎PackageNew capabilitiesTransitivesSizePublishernpm/@types/picomatch@3.0.1None018.1 kBtypes🚮 Removed packages:npm/@types/micromatch@4.0.6View full report↗︎'}
2025-02-01 18:09:55 INFO: Duplicate item skipped: {'id': 'b90abd5b-f152-43ed-807d-ec0f30613fe5', 'type': 'comment', 'author': 'socket-security', 'timestamp': '2024-12-09T22:56:28Z', 'content': 'New and removed dependencies detected.Learn more aboutSocket for GitHub ↗︎PackageNew capabilitiesTransitivesSizePublishernpm/@types/picomatch@3.0.1None018.1 kBtypes🚮 Removed packages:npm/@types/micromatch@4.0.6View full report↗︎'}
2025-02-01 18:09:55 INFO: Finished scraping fixes. Total unique responses: 4
2025-02-01 18:09:55 INFO: Results successfully written to /Users/mitchmcquoid/Desktop/fixes.json

#!/usr/bin/env python3
import os
import requests
from bs4 import BeautifulSoup
import json
import tkinter as tk
import logging
import uuid

# Setup logging to a file in the project root.
project_root = os.path.dirname(os.path.abspath(__file__))
log_file = os.path.join(project_root, "scrape.log")
logging.basicConfig(
    filename=log_file,
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger()

def deduplicate_items(items, key_func):
    """
    Deduplicates a list of dictionary items using a key function.
    """
    unique = {}
    for item in items:
        key = key_func(item)
        if key not in unique:
            unique[key] = item
        else:
            logger.info("Duplicate item skipped: %s", item)
    return list(unique.values())

def scrape_github_issues(url):
    """
    Scrapes a GitHub issues page and extracts details for each issue.
    Returns a JSON-formatted string.
    """
    logger.info("Starting scrape_github_issues for URL: %s", url)
    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/89.0.4389.90 Safari/537.36'
        )
    }
    try:
        response = requests.get(url, headers=headers)
        logger.info("Received response with status code: %d", response.status_code)
    except Exception as e:
        logger.error("Exception during request: %s", e)
        return json.dumps({"error": "Exception during request"}, indent=2)

    if response.status_code != 200:
        logger.error("Failed to fetch page, status code: %d", response.status_code)
        return json.dumps({
            "error": "Failed to fetch page",
            "status_code": response.status_code
        }, indent=2)
    
    soup = BeautifulSoup(response.text, 'html.parser')
    issues = []
    
    # Find all <li> elements representing issue rows.
    issue_rows = soup.find_all("li", class_=lambda x: x and "ListItem-module__listItem" in x)
    logger.info("Found %d issue rows", len(issue_rows))
    
    for row in issue_rows:
        title_elem = row.find("a", class_=lambda x: x and "TitleHeader-module__inline" in x)
        if not title_elem:
            continue
        title = title_elem.get_text(strip=True)
        href = title_elem.get("href", "")
        full_url = "https://github.com" + href if href.startswith("/") else href

        num_elem = row.find("span", class_=lambda x: x and "issue-item-module__defaultNumberDescription" in x)
        number = ""
        if num_elem:
            raw_number = num_elem.get_text(strip=True)
            number = ''.join(filter(str.isdigit, raw_number))
        else:
            logger.debug("No number element found for issue with title: %s", title)

        rel_time_elem = row.find("relative-time")
        created_at = rel_time_elem.get("datetime", "") if rel_time_elem else ""
        
        author_elem = row.find("a", class_=lambda x: x and "issue-item-module__authorCreatedLink" in x)
        author = author_elem.get_text(strip=True) if author_elem else ""
        
        if title and number:
            issues.append({
                "number": number,
                "title": title,
                "url": full_url,
                "created_at": created_at,
                "author": author
            })
        else:
            logger.warning("Skipping issue due to missing title or number: title='%s', number='%s'", title, number)
    
    # Deduplicate based on (number, title)
    issues = deduplicate_items(issues, key_func=lambda item: (item.get("number", ""), item.get("title", "")))
    
    logger.info("Finished scraping issues. Total unique issues: %d", len(issues))
    return json.dumps(issues, indent=2)

def scrape_fix_pages(url):
    """
    Scrapes a GitHub fix page (typically a pull request page) and extracts conversation responses.
    Each response object has: id, type, author, timestamp, and content.
    Returns a JSON-formatted string.
    """
    logger.info("Starting scrape_fix_pages for URL: %s", url)
    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/89.0.4389.90 Safari/537.36'
        )
    }
    try:
        response = requests.get(url, headers=headers)
        logger.info("Received response with status code: %d", response.status_code)
    except Exception as e:
        logger.error("Exception during request in scrape_fix_pages: %s", e)
        return json.dumps({"error": "Exception during request"}, indent=2)

    if response.status_code != 200:
        logger.error("Failed to fetch fix page, status code: %d", response.status_code)
        return json.dumps({
            "error": "Failed to fetch page",
            "status_code": response.status_code
        }, indent=2)
    
    soup = BeautifulSoup(response.text, 'html.parser')
    responses = []
    
    # Look for conversation items in the pull request/fix page.
    conversation_items = soup.find_all("div", class_=lambda x: x and ("timeline-comment" in x or "js-comment-container" in x))
    logger.info("Found %d conversation items", len(conversation_items))
    
    for item in conversation_items:
        author_elem = item.find("a", class_=lambda x: x and "author" in x)
        author = author_elem.get_text(strip=True) if author_elem else "Unknown"
        
        time_elem = item.find("relative-time")
        timestamp = time_elem.get("datetime", "") if time_elem else ""
        
        content_elem = item.find("td", class_=lambda x: x and "comment-body" in x)
        if not content_elem:
            content_elem = item.find("div", class_=lambda x: x and "edit-comment-hide" in x)
        content = content_elem.get_text(strip=True) if content_elem else ""
        
        response_type = "comment"  # Default type for now.
        response_id = str(uuid.uuid4())
        
        if content:
            responses.append({
                "id": response_id,
                "type": response_type,
                "author": author,
                "timestamp": timestamp,
                "content": content
            })
        else:
            logger.debug("Skipping a fix item due to missing content.")
    
    # Deduplicate responses based on (author, content, timestamp) or (author, content) if timestamp is empty.
    responses = deduplicate_items(
        responses,
        key_func=lambda item: (item.get("author", ""), item.get("content", ""), item.get("timestamp", "")) if item.get("timestamp") else (item.get("author", ""), item.get("content", ""))
    )
    
    logger.info("Finished scraping fixes. Total unique responses: %d", len(responses))
    return json.dumps({"responses": responses}, indent=2)

def on_submit():
    logger.info("Submit button pressed.")
    url = url_text.get("1.0", tk.END).strip()
    if not url:
        logger.info("No URL provided; defaulting to issues page.")
        url = "https://github.com/vercel/nft/issues"
    
    mode = mode_var.get()
    logger.info("Mode selected: %s", mode)
    
    output_path_input = output_entry.get().strip()
    if not output_path_input:
        if mode == "issues":
            output_path_input = "~/Desktop/issues"
        elif mode == "fixes":
            output_path_input = "~/Desktop/fixes"
        else:
            output_path_input = "~/Desktop/output"
        logger.info("No output path provided; defaulting to: %s", output_path_input)
    
    output_path = os.path.expanduser(output_path_input)
    if not output_path.endswith(".json"):
        output_path += ".json"
    
    root.destroy()
    
    if mode == "issues":
        output = scrape_github_issues(url)
    elif mode == "fixes":
        output = scrape_fix_pages(url)
    else:
        output = json.dumps({"error": "Unknown mode"}, indent=2)
    
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w") as f:
            f.write(output)
        logger.info("Results successfully written to %s", output_path)
    except Exception as e:
        logger.error("Error writing file: %s", e)

# --------------------- GUI Setup ---------------------
root = tk.Tk()
root.title("GitHub Issues/Fix Pages Scraper")

url_label = tk.Label(root, text="Input page URL from GitHub:")
url_label.pack(pady=(10, 0))
url_text = tk.Text(root, wrap=tk.WORD, width=50, height=4)
url_text.insert(tk.END, "https://github.com/vercel/nft/issues")
url_text.pack(padx=10, pady=10)

output_label = tk.Label(root, text="Specify output file path (e.g., ~/Desktop/filename):")
output_label.pack(pady=(10, 0))
output_entry = tk.Entry(root, width=50)
# Set the default based on the default mode ("issues").
output_entry.insert(tk.END, "~/Desktop/issues")
output_entry.pack(padx=10, pady=10)

mode_var = tk.StringVar(value="issues")
mode_frame = tk.Frame(root)
mode_frame.pack(pady=(10, 0))
radio_issues = tk.Radiobutton(mode_frame, text="Issue Pages", variable=mode_var, value="issues")
radio_issues.pack(side=tk.LEFT, padx=10)
radio_fixes = tk.Radiobutton(mode_frame, text="Fix Pages", variable=mode_var, value="fixes")
radio_fixes.pack(side=tk.LEFT, padx=10)

# Update output file path based on radio button selection.
def update_output_default(*args):
    mode = mode_var.get()
    if mode == "issues":
        default_path = "~/Desktop/issues"
    elif mode == "fixes":
        default_path = "~/Desktop/fixes"
    else:
        default_path = "~/Desktop/output"
    output_entry.delete(0, tk.END)
    output_entry.insert(tk.END, default_path)

# Use trace_add for newer Tkinter versions.
mode_var.trace_add("write", update_output_default)

submit_button = tk.Button(root, text="Submit", command=on_submit)
submit_button.pack(pady=(10, 10))

root.mainloop()
